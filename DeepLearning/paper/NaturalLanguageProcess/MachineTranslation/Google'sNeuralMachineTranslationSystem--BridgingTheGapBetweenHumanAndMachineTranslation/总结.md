3、模型并行化

（1）数据并行：同时训练n个模型的副本，在n个副本之间进行参数共享，每个副本对参数进行异步更新；

（2）模型并行：将网络的不同层跑在不同的gpu上；对softmax层也进行分解，使不同的子集跑在不同的gpu上；

（3）限制：模型并行化对模型架构带来了一些限制，例如：不能在所有的encoder层使用双向网络；不能用decoder的顶层输出进行attention计算，只能用最底层。



三、分段方法：在对oov的词进行翻译时，通常有两种方法，一是copy，基于外部对齐模型的attention，或者负载的特定目标的指向网络；二是子词单元（sub-word units），使用字符、混合词/字符，或者其他子词。

1、本文中最成功的方法是使用子词单元，以数据驱动的方式建立子词模型

2、针对在翻译中需要把某些稀有词直接从输入copy到输出的情况，本文中在源语言和目标语言上使用共享的子词模型来解决。保证了在源语言和目标语言中，相同的字符串以相同的方式被分段，更容易进行copy。

3、子词模型（wordpiece）在字符模型的灵活性和单词模型的有效性之间取得了一个折衷，能够更有效的处理“无限字典”的问题，从而获得更高的BLEU。

4、另外一个解决oov问题的办法是使用混合单词/字符模型：首先像传统模型一样维护一个词典，然后在碰到oov词的时候，将其分解成对应的字符进行处理。



四、训练指标

1、最大似然目标函数

Equation 7

只使用最大似然目标函数，无法反应任务奖励函数（task reward function），会导致模型对于解码过程中出现的错误不具有鲁棒性，因为这些错误在训练过程中没有出现过。

2、本文中，在使用最大似然预训练的模型的基础上，使用task reward进行微调，可以进一步提升性能。

3、强化目标函数

equation 8

在测试时，在单句上直接使用BLEU分数会带来一些不必要的结果（因为BLEU是针对语料库提出的指标），所以本文进行了一些修改，提出了GLEU指标（召回率和准确率的最小值），在语料库级别上具有和BLEU类似的性能，而且在单句上不具有BLEU的缺点。

4、混合目标函数

Equation  9

5、通常会先用最大似然目标函数进行训练，直到收敛；再进一步使用混合目标函数进行微调，直到验证集上的BLEU分数不再变化。